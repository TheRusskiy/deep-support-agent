{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network to behave like a support agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = './data/examples4.txt'\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "# raw_text = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lookup Table\n",
    "To create a word embedding, we first need to transform the words to ids.  In this function, we create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab = [word for word in vocab if counts[word] > 15]\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(vocab)}\n",
    "    last_element = len(vocab)\n",
    "    vocab_to_int['||UNKNOWN||'] = last_element\n",
    "    int_to_vocab[last_element] = '||UNKNOWN||'\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def preprocess_and_save_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    text = load_data(dataset_path)\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    text = text.replace('\\n', ' ').split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    int_lines = [[vocab_to_int.get(word, len(vocab_to_int) - 1) for word in line.split()] for line in lines]\n",
    "    pickle.dump((int_lines, vocab_to_int, int_to_vocab), open('preprocess.p', 'wb'))\n",
    "    \n",
    "preprocess_and_save_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "int_lines, vocab_to_int, int_to_vocab = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "Build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/therusskiy/miniconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input\n",
    "Implementing the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    input_placeholder = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets_placeholder = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    learning_rate_placeholder = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob_placeholder = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return input_placeholder, targets_placeholder, learning_rate_placeholder, keep_prob_placeholder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm_layers = 5\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    named_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    return cell, named_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    named_state = tf.identity(final_state, name=\"final_state\")\n",
    "    return outputs, named_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embeding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embeding)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    return logits, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 9 10 11]\n",
      "   [15 26 26]\n",
      "   [ 4  5 26]]\n",
      "\n",
      "  [[10 11 12]\n",
      "   [26 26  1]\n",
      "   [ 5 26  6]]]\n",
      "\n",
      "\n",
      " [[[12 13 14]\n",
      "   [ 1  2  3]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[13 14 15]\n",
      "   [ 2  3  4]\n",
      "   [ 7  8  0]]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "end_char = vocab_to_int['||END||']\n",
    "def transform_line(line, seq_length):\n",
    "    should_be_length = math.ceil(len(line) / seq_length) * seq_length\n",
    "    split_points = np.arange(seq_length, should_be_length, seq_length)\n",
    "    seqs = np.array_split(line, split_points)\n",
    "    return [np.lib.pad(seq, (0, seq_length - len(seq)), 'constant', constant_values=(0, end_char)) for seq in seqs]\n",
    "    \n",
    "def get_batches(int_lines, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    np.random.shuffle(int_lines)\n",
    "    n_examples = 0\n",
    "    for line in int_lines:\n",
    "        n_examples += math.ceil(len(line) / seq_length)\n",
    "    \n",
    "    n_batches = n_examples // batch_size\n",
    "#     int_lines = np.array([transform_line(np.array(line), seq_length) for line in int_lines])\n",
    "    int_text = np.zeros((n_examples + 1, seq_length), dtype=int)\n",
    "    i = 0\n",
    "    for line in int_lines:\n",
    "        sequences = transform_line(np.array(line), seq_length)\n",
    "        for seq in sequences:\n",
    "            i+=1\n",
    "    i = 0\n",
    "    for line in int_lines:\n",
    "        sequences = transform_line(np.array(line), seq_length)\n",
    "        for seq in sequences:\n",
    "            int_text[i] = seq\n",
    "            i+=1\n",
    "            \n",
    "    int_text = int_text.flatten()\n",
    "    should_have_length = n_batches * 2 * batch_size * seq_length\n",
    "    diff_in_length = should_have_length - len(int_text)\n",
    "    if diff_in_length > 0:\n",
    "        int_text = np.append(int_text, [end_char] * diff_in_length)\n",
    "    \n",
    "    total_elements = n_batches * batch_size * seq_length\n",
    "    result = np.zeros((n_batches, 2, batch_size, seq_length), dtype=int)\n",
    "    for batch_i in range(0, n_batches):\n",
    "        for type_i in range(0, 2):\n",
    "            for subbatch_i in range(0, batch_size):\n",
    "                from_i = seq_length * n_batches * subbatch_i + seq_length * batch_i + type_i\n",
    "                int_range = int_text[from_i:from_i + seq_length]\n",
    "                end_index = np.where(int_range==end_char)[0]\n",
    "#                 if len(end_index) > 0:\n",
    "#                     int_range[end_index[0]:seq_length] = [end_char] * (seq_length - end_index[0])\n",
    "                result[batch_i][type_i][subbatch_i] = int_range\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "example_result_1 = get_batches([[1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11, 12, 13, 14, 15]], 3, 3)\n",
    "print(example_result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 150\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 400\n",
    "# Sequence Length\n",
    "seq_length = 60\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "save_every_n_batches = 500\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr, keep_prob = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size, keep_prob)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(int_lines, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/10562   train_loss = 1.238\n",
      "Saved\n",
      "Epoch   0 Batch  100/10562   train_loss = 1.267\n",
      "Epoch   0 Batch  200/10562   train_loss = 1.266\n",
      "Epoch   0 Batch  300/10562   train_loss = 1.205\n",
      "Epoch   0 Batch  400/10562   train_loss = 1.272\n",
      "Epoch   0 Batch  500/10562   train_loss = 1.369\n",
      "Saved\n",
      "Epoch   0 Batch  600/10562   train_loss = 1.247\n",
      "Epoch   0 Batch  700/10562   train_loss = 1.226\n",
      "Epoch   0 Batch  800/10562   train_loss = 1.304\n",
      "Epoch   0 Batch  900/10562   train_loss = 1.364\n",
      "Epoch   0 Batch 1000/10562   train_loss = 1.241\n",
      "Saved\n",
      "Epoch   0 Batch 1100/10562   train_loss = 1.259\n",
      "Epoch   0 Batch 1200/10562   train_loss = 1.262\n",
      "Epoch   0 Batch 1300/10562   train_loss = 1.229\n",
      "Epoch   0 Batch 1400/10562   train_loss = 1.415\n",
      "Epoch   0 Batch 1500/10562   train_loss = 1.179\n",
      "Saved\n",
      "Epoch   0 Batch 1600/10562   train_loss = 1.210\n",
      "Epoch   0 Batch 1700/10562   train_loss = 1.178\n",
      "Epoch   0 Batch 1800/10562   train_loss = 1.129\n",
      "Epoch   0 Batch 1900/10562   train_loss = 1.237\n",
      "Epoch   0 Batch 2000/10562   train_loss = 1.372\n",
      "Saved\n",
      "Epoch   0 Batch 2100/10562   train_loss = 1.224\n",
      "Epoch   0 Batch 2200/10562   train_loss = 1.232\n",
      "Epoch   0 Batch 2300/10562   train_loss = 1.284\n",
      "Epoch   0 Batch 2400/10562   train_loss = 1.296\n",
      "Epoch   0 Batch 2500/10562   train_loss = 1.284\n",
      "Saved\n",
      "Epoch   0 Batch 2600/10562   train_loss = 1.240\n",
      "Epoch   0 Batch 2700/10562   train_loss = 1.244\n",
      "Epoch   0 Batch 2800/10562   train_loss = 1.234\n",
      "Epoch   0 Batch 2900/10562   train_loss = 1.322\n",
      "Epoch   0 Batch 3000/10562   train_loss = 1.214\n",
      "Saved\n",
      "Epoch   0 Batch 3100/10562   train_loss = 1.227\n",
      "Epoch   0 Batch 3200/10562   train_loss = 1.205\n",
      "Epoch   0 Batch 3300/10562   train_loss = 1.215\n",
      "Epoch   0 Batch 3400/10562   train_loss = 1.287\n",
      "Epoch   0 Batch 3500/10562   train_loss = 1.267\n",
      "Saved\n",
      "Epoch   0 Batch 3600/10562   train_loss = 1.261\n",
      "Epoch   0 Batch 3700/10562   train_loss = 1.368\n",
      "Epoch   0 Batch 3800/10562   train_loss = 1.227\n",
      "Epoch   0 Batch 3900/10562   train_loss = 1.194\n",
      "Epoch   0 Batch 4000/10562   train_loss = 1.266\n",
      "Saved\n",
      "Epoch   0 Batch 4100/10562   train_loss = 1.215\n",
      "Epoch   0 Batch 4200/10562   train_loss = 1.224\n",
      "Epoch   0 Batch 4300/10562   train_loss = 1.143\n",
      "Epoch   0 Batch 4400/10562   train_loss = 1.124\n",
      "Epoch   0 Batch 4500/10562   train_loss = 1.223\n",
      "Saved\n",
      "Epoch   0 Batch 4600/10562   train_loss = 1.166\n",
      "Epoch   0 Batch 4700/10562   train_loss = 1.245\n",
      "Epoch   0 Batch 4800/10562   train_loss = 1.222\n",
      "Epoch   0 Batch 4900/10562   train_loss = 1.217\n",
      "Epoch   0 Batch 5000/10562   train_loss = 1.292\n",
      "Saved\n",
      "Epoch   0 Batch 5100/10562   train_loss = 1.388\n",
      "Epoch   0 Batch 5200/10562   train_loss = 1.268\n",
      "Epoch   0 Batch 5300/10562   train_loss = 1.191\n",
      "Epoch   0 Batch 5400/10562   train_loss = 1.332\n"
     ]
    }
   ],
   "source": [
    "# batches = get_batches(int_lines, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, save_dir)\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                feed = {\n",
    "                    input_text: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: 1,\n",
    "                    initial_state: state,\n",
    "                    lr: learning_rate}\n",
    "                train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "            else:\n",
    "                feed = {\n",
    "                    input_text: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: 0.80,\n",
    "                    initial_state: state,\n",
    "                    lr: learning_rate}\n",
    "                state, _ = sess.run([final_state, train_op], feed)\n",
    "            if (epoch_i * len(batches) + batch_i) % save_every_n_batches == 0:\n",
    "                saver.save(sess, save_dir)\n",
    "                print(\"Saved\")\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e535cb2f38ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mint_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "int_lines, vocab_to_int, int_to_vocab = load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    return loaded_graph.get_tensor_by_name(\"input:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"initial_state:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"final_state:0\"),\\\n",
    "        loaded_graph.get_tensor_by_name(\"keep_prob:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"probs:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def symbol_lookup(word):\n",
    "    table = {\n",
    "        '.': '||DOT||',\n",
    "        ',': '||COMMA||',\n",
    "        ':': '||COLON||',\n",
    "        ';': '||SEMICOLON||',\n",
    "        '-': '||DASH||',\n",
    "        '_': '||UNDERSCORE||',\n",
    "        '!': '||EXCLAMATION||',\n",
    "        '?': '||QUESTION||',\n",
    "        '(': '||LEFTPARENTHESIS||',\n",
    "        ')': '||RIGHTPARENTHESIS||'\n",
    "    }\n",
    "    try:\n",
    "        key = next(key for key, value in table.items() if value == word)\n",
    "        return key\n",
    "    except:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    choice = np.where(probabilities==max(probabilities))[0][0]\n",
    "#     choice = np.random.choice(len(probabilities), 1, p=probabilities)[0]\n",
    "    return int_to_vocab[choice]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conversation\n",
    "\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||ADMIN:|| one hour until your appointment with ||CLIENT_FIRST_NAME|| ||CLIENT_LAST_NAME|| ( ||PHONENUMBER|| . ||ADMIN:|| hi ||GEEK_FIRST_NAME|| , heelotech here . a client in san jose has requested for your service for computer repair & help . pay is ||OTHERPRICE|| . can you help ? ||TECH:|| can you give me the time and date and details of the repair ||START|| ||ADMIN:|| it was one of your previous client , judy arvidson . it's for setting up email on computer . ||END|| ||ADMIN:|| one hour until your appointment with ||CLIENT_FIRST_NAME|| ||CLIENT_LAST_NAME|| ( ||PHONENUMBER|| . ||ADMIN:|| hi ||GEEK_FIRST_NAME|| , heelotech here . a client in san jose has requested for your service for computer repair & help . pay is ||OTHERPRICE|| . can you help ? ||TECH:|| can you give me the time and date and details of the repair ||ADMIN:|| it was one of your previous client , judy arvidson . it's for setting up email on computer . ||TECH:|| okay . sure . i'll take it ||START|| ||ADMIN:|| great ! i'll have the job order assigned to you . kindly contact the client to finalize the appointment . ||END||\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "\n",
    "prime_sentence = \"||ADMIN:|| one hour until your appointment with ||CLIENT_FIRST_NAME|| ||CLIENT_LAST_NAME|| ||LEFTPARENTHESIS|| ||PHONENUMBER|| ||DOT|| ||ADMIN:|| hi ||GEEK_FIRST_NAME|| ||COMMA|| heelotech here ||DOT|| a client in san jose has requested for your service for computer repair & help ||DOT|| pay is ||OTHERPRICE|| ||DOT|| can you help ||QUESTION|| ||TECH:|| can you give me the time and date and details of the repair ||START|| ||ADMIN:|| it was one of your previous client ||COMMA|| judy arvidson ||DOT|| it's for setting up email on computer ||DOT|| ||END|| ||ADMIN:|| one hour until your appointment with ||CLIENT_FIRST_NAME|| ||CLIENT_LAST_NAME|| ||LEFTPARENTHESIS|| ||PHONENUMBER|| ||DOT|| ||ADMIN:|| hi ||GEEK_FIRST_NAME|| ||COMMA|| heelotech here ||DOT|| a client in san jose has requested for your service for computer repair & help ||DOT|| pay is ||OTHERPRICE|| ||DOT|| can you help ||QUESTION|| ||TECH:|| can you give me the time and date and details of the repair ||ADMIN:|| it was one of your previous client ||COMMA|| judy arvidson ||DOT|| it's for setting up email on computer ||DOT|| ||TECH:|| okay ||DOT|| sure ||DOT|| i'll take it ||START|| ||ADMIN:||\"\n",
    "\n",
    "def make_initial_sentences(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    results = []\n",
    "    for word in words:\n",
    "        if word in vocab_to_int:\n",
    "            results.append(word)\n",
    "        else:\n",
    "            results.append('||UNKNOWN||')\n",
    "    return results\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, keep_prob, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = make_initial_sentences(prime_sentence)\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]]), keep_prob: 1.0})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state, keep_prob: 1.0})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "        if pred_word == '||END||':\n",
    "            break\n",
    "    \n",
    "    # Remove tokens\n",
    "    admin_reply = ' '.join([symbol_lookup(word) for word in gen_sentences])\n",
    "    print(admin_reply)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
